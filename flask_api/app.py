import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend before importing pyplot
import io
import os
import re
import string
import logging
import html
import contractions
import nltk
import emoji
import unidecode
import pkg_resources
import pickle
import mlflow
import matplotlib.pyplot   as plt
import matplotlib.dates    as mdates
import numpy               as np
import pandas              as pd
from bs4                   import BeautifulSoup
from symspellpy.symspellpy import SymSpell, Verbosity
from wordcloud             import WordCloud
from nltk.corpus           import stopwords
from nltk.tokenize         import word_tokenize
from nltk.stem             import WordNetLemmatizer, PorterStemmer
from mlflow.tracking       import MlflowClient
from flask_cors            import CORS
from flask                 import Flask, request, jsonify, send_file


app = Flask(__name__)
CORS(app)  # Enable CORS for all routes

# Download required NLTK data for NLP
# nltk.download("punkt")
# nltk.download("stopwords")
# nltk.download("wordnet")

# Initialize SymSpell for spelling correction
sym_spell  = SymSpell(max_dictionary_edit_distance=2)
dict_path  = pkg_resources.resource_filename("symspellpy", "frequency_dictionary_en_82_765.txt")
sym_spell.load_dictionary(dict_path, term_index=0, count_index=1)

# Initialize NLP tools
lemmatizer = WordNetLemmatizer()
stemmer    = PorterStemmer()
# stop_words = set(stopwords.words('english')) - {'not', 'but', 'however', 'no', 'yet'} # retaining important ones for sentiment analysis



# Define clean_comment function
def clean_comment(text):
    """Apply cleaning transformations to a comment."""
    try:
        """Step-by-step text cleaning for NLP tasks."""

        # Step 1Ô∏è‚É£: Convert Tensor to string if needed
        # if isinstance(text, tf.Tensor):
        #     text = text.numpy().decode("utf-8")

        # Step 2Ô∏è‚É£: Remove HTML tags
        text = BeautifulSoup(text, "html.parser").get_text()

        # Step 3Ô∏è‚É£: Expand contractions (e.g., "don't" ‚Üí "do not")
        text = contractions.fix(text)
    
        # Step 4Ô∏è‚É£: Replace hyphens with spaces
        text = re.sub(r"-", " ", text)

        # Step 5Ô∏è‚É£: Remove special characters (except basic punctuation)
        text = re.sub(r"[^a-zA-Z0-9\s.,!?]", "", text)

        # Step 6Ô∏è‚É£: Remove newline characters
        text = text.replace('\n', ' ')

        # Step 7Ô∏è‚É£: Remove URLs
        url_pattern = r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
        text = re.sub(url_pattern, '', text)

        # Step 8Ô∏è‚É£: Convert to lowercase
        text = text.lower()

        # Step 9Ô∏è‚É£: Correct misspellings using SymSpell
        words          = text.split()
        corrected_text = " ".join([
                                    sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)[0].term
                                    if   sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)
                                    else word for word in words
                                ])

        # Step üîü: Remove extra whitespaces
        corrected_text = re.sub(r"\s+", " ", corrected_text).strip()

        # Step 1Ô∏è‚É£1Ô∏è‚É£: Normalize Unicode characters (e.g., accented letters ‚Üí plain text)
        corrected_text = unidecode.unidecode(corrected_text)

        # Step 1Ô∏è‚É£2Ô∏è‚É£: Remove emojis and non-ASCII characters
        corrected_text = emoji.replace_emoji(corrected_text, replace="")

        return corrected_text

    except Exception as e:
        print(f"Error in cleaning comment: {e}")
        return corrected_text

# Define the pre_processing function
def preprocess_comment(cleaned_text):
    """Apply pre_processing transformations to a comment."""
    try:
        """Step-by-step NLP preprocessing: tokenization, stopword removal, lemmatization."""

        # Step 1Ô∏è‚É£: Tokenize the cleaned text
        tokens = word_tokenize(cleaned_text)

        # Step 2Ô∏è‚É£: Remove stopwords
        tokens = [word for word in tokens if word not in stop_words]

        # Step 3Ô∏è‚É£: Lemmatize tokens
        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]

        # Step 4Ô∏è‚É£: (Optional) Stemming ‚Äî currently commented out
        # stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]

        # Step 5Ô∏è‚É£: Reconstruct the processed sentence
        processed_text = " ".join(lemmatized_tokens)

        return processed_text

    except Exception as e:
        print(f"Error in preprocessing comment: {e}")
        return processed_text




# Load the model and vectorizer from the model registry and local storage
def load_model_and_vectorizer(model_name, model_version, vectorizer_path):
    # Set MLflow tracking URI to the server
    mlflow.set_tracking_uri("http://ec2-13-233-244-190.ap-south-1.compute.amazonaws.com:5000/")  # MLflow tracking URI
    client         = MlflowClient()
    model_uri      = f"models:/{model_name}/{model_version}"
    model          = mlflow.pyfunc.load_model(model_uri)
    
    with open(vectorizer_path, 'rb') as file:
        vectorizer = pickle.load(file)
   
    return model, vectorizer


# Initialize the model and vectorizer
# model, vectorizer = load_model_and_vectorizer("yt_chrome_plugin_model", "2", "./tfidf_vectorizer.pkl")  # Update paths and versions as needed

@app.route('/')
def home():
    return "Welcome to our flask api"

@app.route('/predict', methods=['POST'])
def predict():
    data     = request.json
    comments = data.get('comments')
    # print("i am the comment: ",comments)
    # print("i am the comment type: ",type(comments))
    
    if not comments:
        return jsonify({"error": "No comments provided"}), 400

    try:
        # Preprocess each comment before vectorizing
        cleaned_comments      = [clean_comment     (comment) for comment in comments]
        preprocessed_comments = [preprocess_comment(comment) for comment in cleaned_comments]
        
        # Transform comments using the vectorizer
        transformed_comments  = vectorizer.transform(preprocessed_comments)

        # Convert the sparse matrix to dense format
        dense_comments        = transformed_comments.toarray()          # Convert to dense array
        feature_names         = vectorizer.get_feature_names_out()
        dense_df              = pd.DataFrame(dense_comments, columns=feature_names) # to be in sync with signature
        
        # Make predictions
        predictions           = model.predict(dense_df).tolist()       # Convert to list
        
        # Convert predictions to strings for consistency
        # predictions = [str(pred) for pred in predictions]
    except Exception as e:
        return jsonify({"error": f"Prediction failed: {str(e)}"}), 500
    
    # Return the response with original comments and predicted sentiments
    response = [{"comment": comment, "sentiment": sentiment} for comment, sentiment in zip(comments, predictions)]
    return jsonify(response)

@app.route('/predict_with_timestamps', methods=['POST'])
def predict_with_timestamps():
    data          = request.json
    comments_data = data.get('comments')
    
    if not comments_data:
        return jsonify({"error": "No comments provided"}), 400

    try:
        comments              = [item['text']      for item in comments_data]
        timestamps            = [item['timestamp'] for item in comments_data]

        # Preprocess each comment before vectorizing
        cleaned_comments      = [clean_comment     (comment) for comment in comments]
        preprocessed_comments = [preprocess_comment(comment) for comment in cleaned_comments]
        
        # Transform comments using the vectorizer
        transformed_comments  = vectorizer.transform(preprocessed_comments)

        # Convert the sparse matrix to dense format
        dense_comments        = transformed_comments.toarray()          # Convert to dense array
        feature_names         = vectorizer.get_feature_names_out()
        dense_df              = pd.DataFrame(dense_comments, columns=feature_names) # to be in sync with signature
        
        # Make predictions
        predictions           = model.predict(dense_df).tolist()        # Convert to list

        
        # Convert predictions to strings for consistency
        predictions           = [str(pred) for pred in predictions]

    except Exception as e:
        return jsonify({"error": f"Prediction failed: {str(e)}"}), 500
    
    # Return the response with original comments, predicted sentiments, and timestamps
    response = [{"comment": comment, "sentiment": sentiment, "timestamp": timestamp} for comment, sentiment, timestamp in zip(comments, predictions, timestamps)]
    return jsonify(response)

@app.route('/generate_chart', methods=['POST'])
def generate_chart():
    try:
        data             = request.get_json()
        sentiment_counts = data.get('sentiment_counts')
        
        if not sentiment_counts:
            return jsonify({"error": "No sentiment counts provided"}), 400

        # Prepare data for the pie chart
        labels           = ['Positive', 'Neutral', 'Negative']
        sizes            = [
                               int(sentiment_counts.get('1',  0)),
                               int(sentiment_counts.get('0',  0)),
                               int(sentiment_counts.get('-1', 0))
                           ]
        if sum(sizes) == 0:
            raise ValueError("Sentiment counts sum to zero")
        
        colors = ['#36A2EB', '#C9CBCF', '#FF6384']  # Blue, Gray, Red

        # Generate the pie chart
        plt.figure(figsize=(6, 6))
        plt.pie(
                   sizes,
                   labels     = labels,
                   colors     = colors,
                   autopct    = '%1.1f%%',
                   startangle = 140,
                   textprops  = {'color': 'w'}
               )
        plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

        # Save the chart to a BytesIO object
        img_io = io.BytesIO()
        plt.savefig(img_io, format='PNG', transparent=True)
        img_io.seek(0)
        plt.close()

        # Return the image as a response
        return send_file(img_io, mimetype='image/png')
    except Exception as e:
        app.logger.error(f"Error in /generate_chart: {e}")
        return jsonify({"error": f"Chart generation failed: {str(e)}"}), 500

@app.route('/generate_wordcloud', methods=['POST'])
def generate_wordcloud():
    try:
        data     = request.get_json()
        comments = data.get('comments')

        if not comments:
            return jsonify({"error": "No comments provided"}), 400

        # Preprocess comments
        cleaned_comments      = [clean_comment     (comment) for comment in comments]
        preprocessed_comments = [preprocess_comment(comment) for comment in cleaned_comments]
        
        # Combine all comments into a single string
        text                  = ' '.join(preprocessed_comments)

        # Generate the word cloud
        wordcloud             = WordCloud(
                                            width            = 800,
                                            height           = 400,
                                            background_color = 'black',
                                            colormap         = 'Blues',
                                            stopwords        = set(stopwords.words('english')),
                                            collocations     = False
                                         ).generate(text)

        # Save the word cloud to a BytesIO object
        img_io                = io.BytesIO()
        wordcloud.to_image().save(img_io, format='PNG')
        img_io.seek(0)

        # Return the image as a response
        return send_file(img_io, mimetype='image/png')
    except Exception as e:
        app.logger.error(f"Error in /generate_wordcloud: {e}")
        return jsonify({"error": f"Word cloud generation failed: {str(e)}"}), 500

@app.route('/generate_trend_graph', methods=['POST'])
def generate_trend_graph():
    try:
        data             = request.get_json()
        sentiment_data   = data.get('sentiment_data')

        if not sentiment_data:
            return jsonify({"error": "No sentiment data provided"}), 400

        # Convert sentiment_data to DataFrame
        df               = pd.DataFrame(sentiment_data)
        df['timestamp']  = pd.to_datetime(df['timestamp'])

        # Set the timestamp as the index
        df.set_index('timestamp', inplace=True)

        # Ensure the 'sentiment' column is numeric
        df['sentiment']  = df['sentiment'].astype(int)

        # Map sentiment values to labels
        sentiment_labels = {-1: 'Negative', 0: 'Neutral', 1: 'Positive'}

        # Resample the data over monthly intervals and count sentiments
        monthly_counts   = df.resample('M')['sentiment'].value_counts().unstack(fill_value=0)

        # Calculate total counts per month
        monthly_totals   = monthly_counts.sum(axis=1)

        # Calculate percentages
        monthly_percentages = (monthly_counts.T / monthly_totals).T * 100

        # Ensure all sentiment columns are present
        for sentiment_value in [-1, 0, 1]:
            if sentiment_value not in monthly_percentages.columns:
                monthly_percentages[sentiment_value] = 0

        # Sort columns by sentiment value
        monthly_percentages = monthly_percentages[[-1, 0, 1]]

        # Plotting
        plt.figure(figsize=(12, 6))

        colors = {
                     -1: 'red',      # Negative sentiment
                      0: 'gray',     # Neutral  sentiment
                      1: 'green'     # Positive sentiment
                 }

        for sentiment_value in [-1, 0, 1]:
            plt.plot(
                       monthly_percentages.index,
                       monthly_percentages[sentiment_value],
                       marker    = 'o',
                       linestyle = '-',
                       label     = sentiment_labels[sentiment_value],
                       color     = colors[sentiment_value]
                    )

        plt.title('Monthly Sentiment Percentage Over Time')
        plt.xlabel('Month')
        plt.ylabel('Percentage of Comments (%)')
        plt.grid(True)
        plt.xticks(rotation=45)

        # Format the x-axis dates
        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
        plt.gca().xaxis.set_major_locator  (mdates.AutoDateLocator(maxticks=12))

        plt.legend()
        plt.tight_layout()

        # Save the trend graph to a BytesIO object
        img_io = io.BytesIO()
        plt.savefig(img_io, format='PNG')
        img_io.seek(0)
        plt.close()

        # Return the image as a response
        return send_file(img_io, mimetype='image/png')
    except Exception as e:
        app.logger.error(f"Error in /generate_trend_graph: {e}")
        return jsonify({"error": f"Trend graph generation failed: {str(e)}"}), 500
    
@app.route('/health')
def health():
    return jsonify({"status": "ok"})


if __name__ == '__main__':
    try:
        import nltk
        nltk.download("punkt")
        nltk.download("stopwords")
        nltk.download("wordnet")

        from nltk.corpus import stopwords
        stop_words = set(stopwords.words('english')) - {'not', 'but', 'however', 'no', 'yet'}
        print("‚úÖ NLTK data loaded and stopwords initialized")
    except Exception as e:
        print(f"‚ùå NLTK setup failed: {e}")
        stop_words = set()

    try:
        model, vectorizer = load_model_and_vectorizer("yt_chrome_plugin_model", "2", "./tfidf_vectorizer.pkl")
        print("‚úÖ Model and vectorizer loaded successfully")
    except Exception as e:
        print(f"‚ùå Model loading failed: {e}")
        model, vectorizer = None, None

    print("üöÄ Starting Flask app on port 8080...")
    app.run(host='0.0.0.0', port=8080, debug=True)

